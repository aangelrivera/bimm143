---
title: "Class_09_MiniProject"
author: "Angelita Rivera (PID A15522236)"
date: "10/26/2021"
output: pdf_document
---

```{r}

# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

head(wisc.df)
```

Now that we have our data uploaded, we can begin our analysis. 

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]

# Create diagnosis vector for later 
diagnosis <- as.factor(c(wisc.df[,1]))
diagnosis

is.factor(diagnosis)
```

# **Exploratory Data Analysis**

> **Q1.** How many observations are in this dataset? 

```{r}
str(wisc.data)
str(diagnosis)
```

There are 569 observations total in this dataset.

> **Q2.** How many observations have a malignant diagnosis? 

```{r}
table(diagnosis)
```

212 observations have a malignant diagnosis. 

> **Q3.** How many variables/features in the data are suffixed with _mean?

```{r}
wisc.colnames <- c(colnames(wisc.data))
wisc.colnames

grep("_mean", wisc.colnames)
```

There are 10 variables/features suffixed with _mean in this dataset. 

# **Principal Component Analysis**

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)

```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)

# Look at summary of results
summary(wisc.pr)
```

> **Q4.** From your results, what proportion of the original variance is captured by the first principal components (PC1)?

The proportion of original variance captured by the first principal components of PC1 is 0.4427. 

> **Q5.** How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

We need 3 principal components to describe at least 70% of the original variance in the data. 

> **Q6.** How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

We need 7 principal components to describe at least 90% of the original variance in the data. 

# **Interpreting PCA Results**

> **Q7.** What stands out to you about this plot? Is it easy or difficult to understand? Why?

The biplot is not useful for a large data set like we have. It is only useful for a smaller data set (10-15 variables). It uses labels as plots, which, because we have so much data, is near impossible to read. It is way too compacted. We need to create a better plot; something that allows us to plot/view the data in a more clear, readable way. 

```{r}
biplot(wisc.pr)
```

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x, col = diagnosis,
     xlab = "PC1", ylab = "PC2")
```

> **Q8.** Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

The plot is much easier to read. And, it appears to be separated by benign (black) and malignant (red) tumors. 

### ggplot
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col= diagnosis) + 
  geom_point()
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

# **Optional**

```{r}
## ggplot based graph
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

# **Communicating PCA results**

> **Q9.** For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

According to the function above; the component of the loading vector is -0.2608538. 

> **Q10.** What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
var <- summary(wisc.pr)
sum(var$importance[3,] < 0.8)
```

4 principal components are required to explain 80% of the variance data. 

# **Hierarchical clustering**

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
abline(h = 19, col = "red", lty = 2)
```

> **Q11.** Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

The height at which the clustering model has 4 clusters is 19. 

> **Q12.** Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

No, 4 clusters appears to be the best suited number of clusters for this dataset. 

Cut the tree into 4 groups
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

# **Combining methods; Clustering on PCA results**

We take the results of our PCA analysis and cluster in this space 'wisc.pr$x' 

```{r}
summary(wisc.pr)
```

```{r}
wisc.pc.hclust <- hclust( dist(wisc.pr$x[,1:3]), method = "ward.D2")
```

> **Q13.** Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

I liked the method we did above the best because it takes a "bottom up" approach to clustering, which allows us to easily see clearly defined clusters. I thought the method above (and below) gave us the best clustering/plot because the variance was minimized within each cluster. 


Plot my dendrogram 
```{r}
plot( wisc.pc.hclust)
abline(h=60, col = "red")
```

Cut the tree into k=2 groups
```{r}
grps <- cutree(wisc.pc.hclust, k=2)
table(grps)
```

Cross table compare of diagnosis and my cluster groups 
```{r}
table(diagnosis, grps)
```

> **Q15.** How well does the newly created model with four clusters separate out the two diagnoses?

Yes, it does a good job! 


# **Sensitivity/Specificity**

**Accuracy** What proportion did we get correct if we call cluster 1 M and cluster 2 B? 

```{r}
(333+179)/nrow(wisc.data)
```

**Sensitivity** refers to a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples. In other words: TP/(TP+FN).

```{r}
179/(179 + 33)
```

**Specificity** relates to a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign. In other words: TN/(TN+FN).

```{r}
333/(333 + 24)
```

# **Prediction**
```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
```

> **Q17.** Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

The PCA clustering method has the best specificity and sensitivity. 


```{r}
plot(wisc.pr$x[,1:2], col = diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> **Q18.** Which of these new patients should we prioritize for follow up based on your results?

You should prioritize patient #2, because their diagnosis falls within the malignant cluster. 






